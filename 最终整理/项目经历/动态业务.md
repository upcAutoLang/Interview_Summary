# 一. 动态模板

## 1. 业务简要说明

用户有上下级数据的上报体系，所以需要所有数据是有一套数据标准的，标准由甲方定制。然而在实际使用中，用户反映，一方面我们提供的数据不符合他们实际使用情况的内容，另一方面他们实际需要进行记录的数据有一定的可变性，所以用户提出需要根据他们需要，可以动态制定模板的功能。换句话说，就是类似于动态报表，但又与用户信息紧密结合的业务。所以动态模板业务就有了。

## 2. 业务主线流程

业务主要流程分四步。

**第一步是用户在前台填写模板信息。**数据表信息包含表本身的信息，以及各个字段的信息。表本身的信息，基本就是表的名称。字段的信息，有三个是必填字段，数据时间用来记录填入数据的事件、记录 ID 是 UUID 用作主键记录、数据来源 ID 标识了这条数据的来源单位。此外的字段需要用户的需要进行填充，填充的内容包括数据的类型（字符串、数字、时间、数据源、关联表），是否允许筛选，是否允许排序（筛选和排序即对对应的字段建立索引）。比如今天是**<font color=red>蚂蚁金服</font>**的面试，那么新建的表名称就叫做【蚂蚁金服面试】，里面的字段会有时间类型的面试时间、下拉树类型的面试者、字符串形式的面试结果等字段信息。

前端将模板信息构建完毕后，模板信息会传到后台，接下来**第二步就是根据模板信息动态构建数据库表**。模板信息会适配成描述数据库信息的实体类 (DbDefinition)，与实体类配套的，我们构建了一个 FreeMarker 模板，将这个实体类用 FreeMarker 模板渲染后，会渲染出一段数据库 SQL 语句，包含前面生成的所有表信息和字段信息。通过 Mybatis 的 SqlSession，执行这条 SQL 语句，如果不报错的话，那么动态构建数据库表成功，进入下一步。

第三步和第四步是可以并行来做的。在介绍第三步第四步之前，首先先简要介绍一下我们的服务结构。我们通常会有两个服务，一个是服务本身，可以理解成 Controller, Service 层；一个是数据服务，可以理解成 Dao 层。我们拆分为两个业务的原因，在于其他业务组可能会访问我们的业务，所以我们将 Dao 层拆出来，与 Kafka 订阅主题相结合，成为了用于所有业务组访问数据的数据服务。

现在可以对第三步第四步进行一个介绍。**第三步和第四步分别是将模板信息用 HTTP 传输到服务与数据服务上，所有服务解析，并使各自的服务热生效。**第三步和第四步的工作有一个很大程度上的相同工作，就是**动态生成 Class 文件并热生效**。服务接受到模板信息后，适配为描述 JavaBean 信息的实体类（JavaBeanDefinition），然后将 JavaBeanDefinition 实体输入到另外一个 FreeMarker 模板中。这个模板渲染结束后生成一个 \*.java 文件，将这个 Java 文件用系统的编译器（ToolProvider#getSystemJavaCompiler），添加所有 classpath 路径作为编译参数，编译生成 class 文件，同时将 class 文件放在一个 webapp 下指定的存放实体类 \*.class 文件路径下。生成了 \*.class 文件之后，需要对这个路径下的所有 class 文件重新加载。这个路径下的所有 class 文件都是用一个特定的自定义 ClassLoader 加载的，重新加载的方式就是新建一个自定义 ClassLoader 将该路径下的所有类重新加载。上述操作就是服务端和数据服务端的主要操作。

服务端进行了 Java 类的热生效，再修改一些配置，第三步也基本就完成了。但另外一边第四步，也就是数据服务端还需要做一件事情也就是 **Mybatis 热生效**。我们选用的 ORM 框架是 Mybatis，生成了 Java 类并热生效后，也需要令 Mybatis 修改并热生效。同样的，我们还有两个 FreeMarker 模板，用来动态生成 Mybatis 的 Java 接口类，以及 Mybatis 的 XML 文件，生成的文件放到对应的目录下。用同样的渲染方法生成两个文件后，需要让 Mybatis 进行热生效。令 Mybatis 热生效的方式，是将 Mybatis 中 Configuration 的一些信息通过反射的方式清除，然后再重新加载 Mybatis 的所有接口和 XML 文件，这样就实现了 Mybatis 的热生效。

完成了上述所有的步骤，则这一次动态模板发布就完成了。基本的主线流程就是这样。

## 3. 动态模板业务修改字段的实现

### (1) 后端方面

问题所在：如果修改字节码中生成类的字段的话，在调用虚拟机方法 virtualMechine.redefineClasses 时，会抛出异常 "Schema change not implemented"。

### (2) 前端方面

- **作用**：另外，主题服务需要用 JAXB 技术生成 XML，<font color=red>用于网络数据层的前台显示（仅有此作用？）</font>。
- **限制**：
	- 主题端不是我们开发的，他们在生成 XML 的工具使用的是 JAXB，并不是我们经常使用的 DOM4J, XStream 等，所以需要使用他们常用的 JAXB；
	- 需要克服 @XmlSeeAlso 的问题：然而 @XmlSeeAlso 的注解在第一次动态生成 Class 文件的时候就已经生成过了。加载到 ClassLoader 中会出现问题：爆出 "@XmlSeeAlso 已经存在 @XmlSeeAlso" 的错误。克服这个问题的方式，就是重新 new MyClassLoader。
		- 注：@XmlSeeAlso 的作用在后文中。

### (3) 解决方法

解决方法是新建一个 MyClassLoader。  

- 在后端方面，这样绕开了使用同一个 ClassLoader 会令同一个类抛"字段发生改变的异常"的问题；
- 在前端方面，用新的类加载器读取新生成的 Class 数据流，也绕开了前端 @XmlSeeAlso 不能二次生成的问题；

> 如果用户修改了字段（即增删改字段），使用**<font color=red>委托机制，利用 MyClassLoader, HotSwapperClassLoader </font>**判断时间，如果后者的创建时间较之前者更新，则 new MyClassLoader，用新的 ClassLoader 生成新修改字段后的 Class 文件。

### (4) @XmlSeeAlso 的作用

由于主题端代码中有泛型继承的内容，所以由于泛型擦除的特性，JAXB 提供了 @XmlSeeAlso 的 Class 级别注解，用于标注说明该泛型类型应该在 @XmlSeeAlso 的 Class[] 列表中。即 @XmlSeeAlso 是用来记录主题端 DataSet\<T> 的泛型内容。

## 4. 业务并发量与数据量



# 二. 动态汇总统计

## 1. 业务简要说明

前面的动态模板只是负责数据的来源，动态汇总统计业务负责的是对数据的动态统计。这个业务诞生的原因也比较类似。甲方给的统计也是按照一定标准的，实际使用的用户认为标准给的统计内容并不符合他们的期望，所以希望我们有一种动态汇总统计的能力，用于他们的统计、文本导出、数据展现。

## 2. 业务设计

这个项目我负责的是全部设计，以及核心功能的后端实现。

首先，我们的业务统计面向的主要是表格。表格统计的内容主要有什么呢？主要有两部分：一部分是表格本身的数据内容，另一部分是对表格数据进行一段话的概述信息统计。

- **表格数据的内容主要是按照数据列进行处理**，所有数据行的所有列处理完毕后，就拼成了一个汇总表格。这里面，对数据列进行处理的方式有简单有复杂。
  - 简单处理方式比如提取字段，无需处理，直接从一条数据中获取对应的字段，结果就是一个数据列；
  - 复杂处理方式比如对一条数据的物资信息进行统计，物资信息是数据行对应数据的子表信息，那么就要遍历物资子列表，统计信息串成字符串。
- **一段话的概述统计主要是对表格数据进行处理**，遍历所有表格的所有行列，统计得出一段话来总结说明数据列表的内容。

在上面的业务情景的前提下，我定义了一个**维度**的概念：表格是二维，对应 JSONArray；数据行是一维，对应 JSONObject；一个字符串（字符串、整数、浮点数）是零维，对应 String。所以，无论是对一条一维行数据按照字段进行处理，结果产出一个字段的字符串内容，还是对二维表格进行汇总处理，结果产出一段总结性的统计信息，本质上都是高维向零维的处理过程。处理的过程，通常是对输入进行<font color=red>**顺序的、链式的**</font>方式进行的，这条处理链的每个处理单元，输入和输出都是有严格限定的维度。举两个例子：

- 比如从数据行中提取一个 key-value 作为输出的处理单元，就是一维输入，零维输出；
- 比如从数据行中提取出其子表信息的处理单元，就是一维输入，二维输出；

其他还有筛选、排序、累加计算等处理单元，都有自己允许的输入输出维度。一串<font color=red>**匹配输入输出维度的**</font>处理单元串联起来就构成了一条处理链。

上面就是动态汇总统计业务整个**设计层面**的思路。

## 3. 具体实现

那么用户如何进行配置，来实现对几张数据表进行自定义的报表统计呢？

在配置中心的前台页面，构建一个报表统计项，并为该统计项选择对应的若干张数据表。一个报表统计项就初始化完成了。然后开始进行各字段的配置。每添加一个数据列，为其构建处理链，然后向处理链中逐个添加处理单元。添加处理单元有一定的限制，其中最大的限制就是前后处理单元的匹配，**前一个单元的输出必须和后一个单元的输入匹配**。这样构建结束一个数据列后，再逐个构建其他数据列，就形成了一个报表统计项。同理对于一个报表统计项的一段话统计，也是用处理链的形式进行的。和前面不同的是，构建字段的处理链输入是一维，输出是零维；而表格的一段话统计，输入二维，输出是零维。

报表统计项构建完毕后，将配置信息传到后台服务，对配置信息的可用性和安全性进行检查后，将配置信息入库。入库是以字符串的形式进行的，具体的方法在后面再解释。

用户实际查询报表统计项内容的时候：

1. 首先，发起查询请求，后台一方面从数据库中查数据表的数据，另一方面查询配置信息；
2. 然后，后台将字符串形式的配置信息转成处理单元列表；
3. 最后，将数据输入处理链，最后生成一个 JSONArray 类型的处理结果，交给前台渲染，即为用户所得。

至于字符串形式的配置信息，转成处理单元的方式，我是使用了 Google 的一个开源表达式处理引擎完成的（名为 Aviator），这个表达式引擎有一个全局静态的方法注册中心，用来存储一些处理方法。我们的服务启动的时候，我将一系列自定义的提取、筛选、排序等处理方法注册到表达式引擎中。然后使用过程中，用户添加一个新的报表统计项时，后台将配置信息适配成表达式引擎的字符串格式，存入数据库。用户在访问该报表统计项时，对表达式进行解析，然后匹配表达式中已经被注册的方法，提取方法进行处理。

上面就是动态汇总统计的主线流程实现。

------

**性能处理：**  

当然用户实际使用的时候，为了保证汇总服务的性能，主要还是依托于 Kafka 的订阅还有 Redis 缓存。对于一个报表统计项，一旦触发了其中涉及的某一数据表的增删改操作，我们的数据服务会同时发布订阅消息，通知我们的汇总服务，然后汇总服务将与该数据表相关的所有统计项的缓存清除。用户在访问报表统计项时，后台将会处理汇总前端前三页的数据内容，并存入 Redis，提高访问性能。