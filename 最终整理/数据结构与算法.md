# 一. HashMap 与 ConcurrentHashMap

> 参考地址：  
> [《Java 8 ConcurrentHashMap源码分析》](https://www.jianshu.com/p/cf5e024d9432)  
> [《Java 8 HashMap源码分析》](https://www.jianshu.com/p/aa017a3ddc40)  
> [《《吊打面试官》系列-HashMap》](https://juejin.im/post/5dee6f54f265da33ba5a79c8)
> [《《吊打面试官》系列-ConcurrentHashMap & Hashtable》](https://juejin.im/post/5df8d7346fb9a015ff64eaf9)  
> [《关于ConcurrentHashMap1.8的个人理解》](https://segmentfault.com/a/1190000015907000)

## 1.1 HashMap

## 1.2 ConcurrentHashMap

### 线程安全方法修改的原因

> 参考地址：[《ConcurrentHashMap 1.8为什么要使用CAS+Synchronized取代Segment+ReentrantLock》](https://www.cnblogs.com/yangfeiORfeiyang/p/9694383.html)

ConcurrentHashMap 的关键方法 <code>putVal()</code> 中，依旧使用**数组 + 链表**的思想来说实现，同时以数组的每个**桶 (Bucket)** 为锁目标。  
在 JDK 1.7 版本，ConcurrentHashMap 是通过 **Segment + ReentrantLock** 实现锁的实现的。而到了 JDK 1.8 版本，ConcurrentHashMap 改成了 **CAS + synchronized**方式。  
在讨论修改线程安全方法原因前，需要理解的知识点：

- CAS；
- synchronized, 重入锁 ReentrantLock 的区别；见[《并发与 Java 基础》篇](./并发与Java基础.md)
- synchronized 的锁优化：偏向锁、轻量级锁、自旋锁、重量级锁；见[《JVM》篇](./JVM.md)

#### JDK 1.7 版本

**1.7 版本保证线程并发安全**：使用继承了 ReentrantLock 的 **Segment**。   
在初始化 ConcurrentHashMap 时，首先初始化一个容量为 16 的 Segment 数组，每个 Segment 都是一个锁。每个 Segment 内部有一个 HashEntry 类型的 <code>table</code> 数组，每个 table 数组里的索引数据又对应一个 Node 链表。数据结构类似如图：

![JDK1.7 ConcurrentHashMap 数据结构](./pic/JDK1.7_ConcurrentHashMap_数据结构.png)

这样做的好处是**降低了并发量**。如果进行 put 操作将整个 ConcurrentHashMap 锁定，put 效率一定十分低下。所以用 Segment 数组中的元素作为锁粒度，对 Key 进行 Hash 运算取模找到对应的 Segment，对该 Segment 进行上锁，由于数组中所有的 Segment 元素都是独立分开的，一个上锁不会影响另外一个，这样就把锁住全表再进行 put 的效率提高了至少 16 倍。

#### JDK 1.8 版本

**1.8 版本保证线程并发安全**：使用了 **CAS + synchronized 关键字**的方法。1.8 版本的 ConcurrentHashMap 的数据节点不再挂在 Segment 下面，而是直接管理 Node 类型的 <code> table</code> 数组。   
**putVal()** 方法中该部分的关键代码如下：

```java
    final V putVal(K key, V value, boolean onlyIfAbsent) {
        // ....
        
        for (Node<K,V>[] tab = table;;) {
            Node<K,V> f; int n, i, fh;
            if (tab == null || (n = tab.length) == 0)
                // ...
            else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
                // ...
            }
            // ...
            else {
                V oldVal = null;
                synchronized (f) {
                    // ...
                }
            }
        }
        // ...
    }
```

关键部分在于 <code>synchronized (f)</code> 部分。上锁内容 f 就是 table 数组中一个 Node 元素。它的锁细化思路与 JDK 1.7 版本的相同。

1.8 版本与 1.7 版本的思路相同，那么优化原因与优化内容是什么呢？主要区别在于两种锁 **已优化的 synchronized 关键字** 与**重入锁 ReentrantLock**。

> synchronized 关键字的优化，见[《JVM》篇](./JVM.md)  
> synchronized 关键字与重入锁 ReentrantLock 的区别与联系，见[《并发与Java基础》篇](./并发与Java基础.md)

通过锁粒度细化的思想，出现并发争抢的可能性已经很小了。所以主要讨论的就是出现争抢的情况。  
ConcurrentHashMap 在 1.7 版本中是通过 Segment 实现的，Segment 继承于 ReentrantLock，拥有重入锁的特性。而 ReentrantLock 锁的实现原理是通过 CAS 的 <code>park()</code> 方法实现了线程的挂起，这样会导致线程**用户态与内核态的切换**，相当于**发生争抢时直接使用了重量级锁**。  
到了 1.8 版本，ConcurrentHashMap 使用了 synchronized 关键字。由于 synchronized 关键字已经在 JDK 层面进行了优化，**发生争抢的情况下**，synchronized 关键字会**从偏向锁升级为自旋锁**。所以在争抢状态下，只要线程可以在 30-50 次之内自旋时间内拿到锁，synchronized 关键字就不会升级为重量级锁，这样就减少了挂起和唤醒的线程上下文切换的大量开销。  
所以如果在线程并发量不大的情况下，synchronized 关键字由于偏向锁、自旋锁优化不用挂起线程的原因，效率比 ReentrantLock 高效。

## 1.3 其他问题

问题：**集合的种类和区别，HashMap 底层如何实现 (JDK1.8有所改动)，HashMap 和 ConcurrentHashMap 的区别？**

HashMap 在 1.8 之前是使用数组 + 链表的形式实现的，1.8 进行了改动，当链表深度小于 8 的时候，依旧使用链表进行第二级存储；大于 8 时，将链表形式转为了红黑树模式。  
**查询**:  

1. 计算 Key 的 Hash 值，并与 length 取模，计算得到 bucket 的位置；
2. 在该 bucket 的位置上进行查询；查询分成两种情况：
	- 使用链表查询：长度小于 8，查询时间复杂度 O(n)；
	- 使用红黑树查询：长度大于 8，查询时间复杂度 O(n\*logn)；

**插入**:  

1. 计算 Hash 值，与 length 取模计算得到 bucket 位置；
2. 判断使用链表形式插入或是使用红黑树进行插入；
	- 链表插入：访问 bucket 所在位置的链表，在尾部插入元素；
	- 红黑树插入：
3. 插入完毕后，需要进行判断：
	- **是否需要扩容？**
	- **是否需要转换当前 bucket 的链表为红黑树？**

ConcurrentHashMap 和 HashMap 的基本实现原理相同，但加入了并发控制。并发控制的粒度在数组的元素级别上 (table 的 index 上)。

# 二. 关于二叉查找树、平衡二叉树 (AVL)，B 树的思想

研究 B+ Tree 时发现，B+ Tree 的思想是逐步演化而来的。由二叉查找树 -> 平衡二叉树 -> 2-3 树 -> B 树演变而来；

## 1. 二叉查找树

[《浅谈算法和数据结构: 七 二叉查找树》](http://www.cnblogs.com/yangecnu/p/Introduce-Binary-Search-Tree.html)

二叉查找树是一个很典型的递归概念：

- 节点 root 的左子树上的任意节点的 key 值都比 root 小；
- 节点 root 的右子树上的任意节点的 key 值都比 root 大；
- root 的左右子树都是二叉查找树；

二叉树的查找效率与二叉树的结构有关，如果是平衡的二叉树，查找效率可以达到 logN，但极端条件下（如只有右子树的情况），则退化成单链表。

注：二叉树的根节点，是第一个插入到二叉树的节点，后续插入删除过程不会更新根节点；

## 2. 平衡二叉树 (AVL)

## 3. 2-3 树

[《浅谈算法和数据结构: 八 平衡查找树之2-3树》](http://www.cnblogs.com/yangecnu/p/Introduce-2-3-Search-Tree.html)

> 注：2-3 树是一种比较复杂的数据结构，理解 2-3 树的主要作用，是为了理解红黑树。红黑树是对于 2-3 树的编码实现，并将 2-3 转换为二叉树的形式。

2-3 树最重要的性质就是**插入过程中实现自平衡。**对于插入操作，可以分为两种情况：

- **向 2 节点插入**：只需要将 2 节点变为 3 节点即可（且 3 节点内部按照两个 key 值大小也有排序）；
- **向 3 节点插入**：先插入到 3 节点中变为 4 节点，然后将 4 节点拆开，中间两个元素  (记作 2, 3) 抽出作为父节点，1 作为父节点的左子树，4 作为父节点的右子树；
	- 如果涉及到多层 3 节点的插入，则一直分裂，直到遇到 2 节点，或者根节点为止；
	- **红黑树的<font color=red>旋转操作</font>，本质上就是 2-3 树插入操作的过程中，为了保证 2-3 树的性质而进行的补充操作**；

<img src="https://images0.cnblogs.com/blog/94031/201403/252248589202242.png" alt="插入三节点" style="zoom:75%;" />



## 4. 红黑树

[《浅谈算法和数据结构: 九 平衡查找树之红黑树》](http://www.cnblogs.com/yangecnu/p/Introduce-Red-Black-Tree.html)

红黑树原本感觉很难，但发现本质上就是一个**用红色表示 2 节点的 2-3 树**，形式上仍旧使用普通二叉树的形式。这样既有了二叉树的操作，又有 2-3 树的高效率查询（logN）；

注：2-3 树插入时的自平衡操作，被转换为红黑树的**左旋、右旋、翻色 (filpColor)** 操作；

# 三. 如何理解 BTree 机制？

[《浅谈算法和数据结构: 十 平衡查找树之B树》](http://www.cnblogs.com/yangecnu/p/Introduce-B-Tree-and-B-Plus-Tree.html)  
[《MySQL索引背后的数据结构及算法原理》](http://blog.codinglabs.org/articles/theory-of-mysql-index.html)

## 3.1 B-Tree 与 B+Tree

B-Tree 是 2-3 树的一种变形，可以设置度数 M，每个节点上最多可以有 M 个值；根据硬盘读取时的预读原理，磁盘读取时每次从磁盘上预读 page 容量（一般为 1024 字节）的整数倍，所以对于硬盘来说，可以将度设为 1024，这样就硬盘文件的索引就建立完毕，形成了一个 B-Tree 结构；

B-Tree 与 B+Tree 的区别在于：

- **节点的不同**
	- B-Tree 的子节点都是相同的。无论叶子节点还是非叶子结点，上面存储的数据都是索引 + 数据；
	- B+Tree 的叶子节点和非叶子结点存储内容不同，非叶子节点只存储索引信息，叶子节点之存储具体数据信息；
- 索引与数据分离
	- B-Tree 各节点的索引和数据是放在一起的；
	- B+Tree 的索引和数据分离开来，所以非叶子结点只占用了很小一部分的空间，大部分空间被存储数据信息的叶子节点占据；
- 数据结构
	- B-Tree 是一个比较单纯的树结构；所以 B-Tree 结构的同一个父节点的相邻叶子节点，在物理上很可能是相距很远的；
	- B+Tree 在 B-Tree 的基础树结构之上，在叶子节点之间添加了指针连接，使所有叶子节点形成了一个**链表结构，提升了叶子节点的顺序读取效率**。

## 3.2 B+Tree 的优势

为什么 B+Tree 更加适合数据库、文件系统的存取模型？首先需要了解两个概念：

1. **磁盘结构**：磁盘的物理结构是若干个同心圆组成的，每个同心圆都叫做磁道，每个同心圆按照一定的角度切分即为扇区。磁盘读取数据的过程，首先从地址总线获取数据存储的物理地址位置，然后磁头转动到相应的磁道上，该过程称为**寻道时间**；然后磁盘转动，磁头相对移动到数据在磁道上的位置，即到达了数据所在地址，该过程称为**旋转时间**；
2. **局部性原理**：一个数据被访问的时候，其周围相邻的数据通常也需要被使用到。这是一个定理。
3. **磁盘预读**：磁盘顺序读取的效率很高，因为顺序读取在同一个磁道上的数据，所以不需要寻道时间，只需要少量的旋转时间即可。所以根据局部性原理，磁盘在读取一个数据的时候，往往会将该数据周围范围内的数据也一并读取，即磁盘预读。

所以 B+Tree 在文件管理和数据库存储方面的优势就体现出来了：

1. B-Tree, B+Tree 的查询、插入、删除的时间复杂度都是 logN 级别，相对足够高效；
2. B+Tree 相对于 B-Tree 多了叶子节点的链表结构，所以在顺序读取的效率上是远高于 B-Tree 的；这在磁盘预读原理之下，成为十分重要的优势；
3. 磁盘预读的大小一般为一个页 (Page) 的整数倍，所以在 B+Tree 中将节点的度设置为一个 Page 的大小，则每次新建一个节点时都会将磁盘预读的数据信息存储到一个叶子节点中，相应索引信息存储到非叶子结点中。

## 3.3 B+ 树索引和 Hash 索引的区别

[《MySQL B+树索引和哈希索引的区别》](https://www.cnblogs.com/heiming/p/5865101.html)

Hash 索引和 Java 的 Hash 原理基本相同，属于数组 + 链表 / 数组 + 红黑树的搜索方法。但和 B+ 比较有较少优点，较多缺点：

- 优点：
	- 更加适合**等值查找**。Hash 查询的效率本来就是 O(1)，如果所有 key-value 的 Hash 值都不同，则 Hash 索引查询的效率应该远胜于 B+ Tree；
- 缺点：
	- **不适合范围查找**。key-value 经过 Hash 的计算，就会变的不连续，在范围查找上没有丝毫用处；
	- **不适合模糊查找**。即使用 "LIKE %...%" 的语句，本质上和不适合范围查找是一样的；
	- **不适合大量等值的情况**。即大量重复键值的情况下，key-value 计算出来的 Hash 值相同，这时候相同 Hash 值的内容形成链表，极端情况下复杂度变为 O(n)；（Hash 碰撞问题）
	- <font color=red>不支持多列联合索引的最左匹配规则</font>

# 四. treemap和HashMap的区别？

首先介绍一下什么是Map。在数组中我们是通过数组下标来对其内容索引的，而在Map中我们通过对象来对对象进行索引，用来索引的对象叫做key，其对应的对象叫做value。这就是我们平时说的键值对。

HashMap通过hashcode对其内容进行快速查找，而 TreeMap中所有的元素都保持着某种固定的顺序，如果你需要得到一个有序的结果你就应该使用TreeMap（HashMap中元素的排列顺序是不固定的）。

## 1. HashMap 非线程安全 TreeMap 非线程安全

在Java里，线程安全一般体现在两个方面：

1. 多个thread对同一个java实例的访问（read和modify）不会相互干扰，它主要体现在关键字synchronized。如ArrayList和Vector，HashMap和Hashtable
（后者每个方法前都有synchronized关键字）。如果你在interator一个List对象时，其它线程remove一个element，问题就出现了。
2. 每个线程都有自己的字段，而不会在多个线程之间共享。它主要体现在java.lang.ThreadLocal类，而没有Java关键字支持，如像static、transient那样。

## 2. AbstractMap抽象类和SortedMap接口

- AbstractMap 抽象类：(HashMap继承AbstractMap)覆盖了equals()和hashCode()方法以确保两个相等映射返回相同的哈希码。如果两个映射大小相等、包含同样的键且每个键在这两个映射中对应的值都相同，则这两个映射相等。映射的哈希码是映射元素哈希码的总和，其中每个元素是Map.Entry接口的一个实现。因此，不论映射内部顺序如何，两个相等映射会报告相同的哈希码。
- SortedMap 接口：（TreeMap继承自SortedMap）它用来保持键的有序顺序。SortedMap接口为映像的视图(子集)，包括两个端点提供了访问方法。除了排序是作用于映射的键以外，处理SortedMap和处理SortedSet一样。添加到SortedMap实现类的元素必须实现Comparable接口，否则您必须给它的构造函数提供一个Comparator接口的实现。TreeMap类是它的唯一一份实现。

## 3. 两种常规Map实现

### (1) HashMap

HashMap：基于哈希表实现。使用HashMap要求添加的键类明确定义了hashCode()和equals()[可以重写hashCode()和equals()]，为了优化HashMap空间的使用，您可以调优初始容量和负载因子。

(1)HashMap(): 构建一个空的哈希映像.  
(2)HashMap(Map m): 构建一个哈希映像，并且添加映像m的所有映射.  
(3)HashMap(int initialCapacity): 构建一个拥有特定容量的空的哈希映像.  
(4)HashMap(int initialCapacity, float loadFactor): 构建一个拥有特定容量和加载因子的空的哈希映像  

### (2) TreeMap

TreeMap：基于红黑树实现。TreeMap没有调优选项，因为该树总处于平衡状态。

(1)TreeMap():构建一个空的映像树.  
(2)TreeMap(Map m): 构建一个映像树，并且添加映像m中所有元素.  
(3)TreeMap(Comparator c): 构建一个映像树，并且使用特定的比较器对关键字进行排序.  
(4)TreeMap(SortedMap s): 构建一个映像树，添加映像树s中所有映射，并且使用与有序映像s相同的比较器排序.  

### 4. 两种常规 Map 性能

HashMap：适用于在Map中插入、删除和定位元素。
Treemap：适用于按自然顺序或自定义顺序遍历键(key)。

### 5. 总结

HashMap通常比TreeMap快一点(树和哈希表的数据结构使然)，建议多使用HashMap，在需要排序的Map时候才用TreeMap。

```java
import java.util.HashMap;
import java.util.Hashtable;
import java.util.Iterator;
import java.util.Map;
import java.util.TreeMap;
public class HashMaps {
    public static void main(String[] args) {
        Map<String, String> map = new HashMap<String, String>();
        map.put("d", "ddd");
        map.put("b", "bbb");
        map.put("a", "aaa");
        map.put("c", "ccc");
        Iterator<String> iterator = map.keySet().iterator();
        while (iterator.hasNext()) {
            Object key = iterator.next();
            System.out.println("map.get(key) is :" + map.get(key));
        }
        // 定义HashTable,用来测试 
        Hashtable<String, String> tab = new Hashtable<String, String>();
        tab.put("d", "ddd");
        tab.put("b", "bbb");
        tab.put("a", "aaa");
        tab.put("c", "ccc");
        Iterator<String> iterator_1 = tab.keySet().iterator();
        while (iterator_1.hasNext()) {
            Object key = iterator_1.next();
            System.out.println("tab.get(key) is :" + tab.get(key));
        }
        TreeMap<String, String> tmp = new TreeMap<String, String>();
        tmp.put("d", "ddd");
        tmp.put("b", "bbb");
        tmp.put("a", "aaa");
        tmp.put("c", "ccc");
        Iterator<String> iterator_2 = tmp.keySet().iterator();
        while (iterator_2.hasNext()) {
            Object key = iterator_2.next();
            System.out.println("tmp.get(key) is :" + tmp.get(key));
        }
    }
}
```

运行结果如下：

```txt
 map.get(key) is :ddd
 map.get(key) is :bbb
 map.get(key) is :ccc
 map.get(key) is :aaa
 tab.get(key) is :bbb
 tab.get(key) is :aaa
 tab.get(key) is :ddd
 tab.get(key) is :ccc
 tmp.get(key) is :aaa
 tmp.get(key) is :bbb
 tmp.get(key) is :ccc
 tmp.get(key) is :ddd
```

HashMap的结果是没有排序的，而TreeMap输出的结果是排好序的。

下面就要进入本文的主题了。先举个例子说明一下怎样使用HashMap:

```java
import java.util.*;
public class Exp1 {
    public static void main(String[] args){
        HashMap h1=new HashMap();
        Random r1=new Random();
        for (int i=0;i<1000;i++){
            Integer t=new Integer(r1.nextInt(20));
            if (h1.containsKey(t))
                ((Ctime)h1.get(t)).count++;
            else
                h1.put(t, new Ctime());
        }
        System.out.println(h1);
    }
}
class Ctime{
    int count=1;
    public String toString(){
        return Integer.toString(count);
    }
}
```

在HashMap中通过get()来获取value，通过put()来插入value，ContainsKey()则用来检验对象是否已经存在。可以看出，和ArrayList的操作相比，HashMap除了通过key索引其内容之外，别的方面差异并不大。

前面介绍了，HashMap是基于HashCode的，在所有对象的超类Object中有一个HashCode()方法，但是它和equals方法一样，并不能适用于所有的情况，这样我们就需要重写自己的HashCode()方法。下面就举这样一个例子：

```java
import java.util.*;
public class Exp2 {
    public static void main(String[] args){
        HashMap h2=new HashMap();
        for (int i=0;i<10;i++)
            h2.put(new Element(i), new Figureout());
        System.out.println("h2:");
        System.out.println("Get the result for Element:");
        Element test=new Element(5);
        if (h2.containsKey(test))
            System.out.println((Figureout)h2.get(test));
        else
            System.out.println("Not found");
    }
}
class Element{
    int number;
    public Element(int n){
        number=n;
    }
}
class Figureout{
    Random r=new Random();
    boolean possible=r.nextDouble()>0.5;
    public String toString(){
        if (possible)
            return "OK!";
        else
            return "Impossible!";
    }
}
```

在这个例子中，Element用来索引对象Figureout,也即Element为key，Figureout为value。在Figureout中随机生成一个浮点数，如果它比0.5大，打印”OK!”，否则打印”Impossible!”。之后查看Element(3)对应的Figureout结果如何。

结果却发现，无论你运行多少次，得到的结果都是”Not found”。也就是说索引Element(3)并不在HashMap中。这怎么可能呢？

原因得慢慢来说：Element的HashCode方法继承自Object，而Object中的HashCode方法返回的HashCode对应于当前的地址，也就是说对于不同的对象，即使它们的内容完全相同，用HashCode（）返回的值也会不同。这样实际上违背了我们的意图。因为我们在使用 HashMap时，希望利用相同内容的对象索引得到相同的目标对象，这就需要HashCode()在此时能够返回相同的值。在上面的例子中，我们期望 new Element(i) (i=5)与 Elementtest=newElement(5)是相同的，而实际上这是两个不同的对象，尽管它们的内容相同，但它们在内存中的地址不同。因此很自然的，上面的程序得不到我们设想的结果。下面对Element类更改如下：

```java
class Element{
    int number;
    public Element(int n){
        number=n;
    }
    public int hashCode(){
        return number;
    }
    public boolean equals(Object o){
        return (o instanceof Element) && (number==((Element)o).number);
    }
}
```

在这里Element覆盖了Object中的hashCode()和equals()方法。覆盖hashCode()使其以number的值作为 hashcode返回，这样对于相同内容的对象来说它们的hashcode也就相同了。而覆盖equals()是为了在HashMap判断两个key是否相等时使结果有意义（有关重写equals()的内容可以参考我的另一篇文章《重新编写Object类中的方法》）。修改后的程序运行结果如下：

```txt
h2:
Get the result for Element:
Impossible!
```

请记住：如果你想有效的使用HashMap，你就必须重写在其的HashCode()。

还有两条重写HashCode()的原则：
[list=1]

不必对每个不同的对象都产生一个唯一的hashcode，只要你的HashCode方法使get()能够得到put()放进去的内容就可以了。即”不为一原则”。

生成hashcode的算法尽量使hashcode的值分散一些，不要很多hashcode都集中在一个范围内，这样有利于提高HashMap的性能。即”分散原则”。至于第二条原则的具体原因，有兴趣者可以参考Bruce Eckel的《Thinking in Java》，在那里有对HashMap内部实现原理的介绍，这里就不赘述了。

掌握了这两条原则，你就能够用好HashMap编写自己的程序了。不知道大家注意没有，java.lang.Object中提供的三个方法：clone()，equals()和hashCode()虽然很典型，但在很多情况下都不能够适用，它们只是简单的由对象的地址得出结果。这就需要我们在自己的程序中重写它们，其实java类库中也重写了千千万万个这样的方法。利用面向对象的多态性——覆盖，Java的设计者很优雅的构建了Java的结构，也更加体现了Java是一门纯OOP语言的特性。

# 五. Hash 的一致算法

## 1. 一致性Hash算法背景

一致性哈希算法在1997年由麻省理工学院的Karger等人在解决分布式Cache中提出的，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简单哈希算法带来的问题，使得DHT可以在P2P环境中真正得到应用。  
但现在一致性hash算法在分布式系统中也得到了广泛应用，研究过memcached缓存数据库的人都知道，memcached服务器端本身不提供分布式cache的一致性，而是由客户端来提供，具体在计算一致性hash时采用如下步骤：

1. 首先求出memcached服务器（节点）的哈希值，并将其配置到0～232的圆（continuum）上。
2. 然后采用同样的方法求出存储数据的键的哈希值，并映射到相同的圆上。
3. 然后从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务器上。如果超过232仍然找不到服务器，就会保存到第一台memcached服务器上。

![](https://images2015.cnblogs.com/blog/498077/201608/498077-20160822172408386-366341651.png)

从上图的状态中添加一台memcached服务器。余数分布式算法由于保存键的服务器会发生巨大变化而影响缓存的命中率，但Consistent Hashing中，只有在园（continuum）上增加服务器的地点逆时针方向的第一台服务器上的键会受到影响，如下图所示：

![](https://images2015.cnblogs.com/blog/498077/201608/498077-20160822172431933-546286787.png)

## 2. 一致性Hash性质

考虑到分布式系统每个节点都有可能失效，并且新的节点很可能动态的增加进来，如何保证当系统的节点数目发生变化时仍然能够对外提供良好的服务，这是值得考虑的，尤其实在设计分布式缓存系统时，如果某台服务器失效，对于整个系统来说如果不采用合适的算法来保证一致性，那么缓存于系统中的所有数据都可能会失效（即由于系统节点数目变少，客户端在请求某一对象时需要重新计算其hash值（通常与系统中的节点数目有关），由于hash值已经改变，所以很可能找不到保存该对象的服务器节点），因此一致性hash就显得至关重要，良好的分布式cahce系统中的一致性hash算法应该满足以下几个方面：

- **平衡性 (Balance)**: 平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。
- **单调性 (Monotonicity)**: 单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲区加入到系统中，那么哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去，而不会被映射到旧的缓冲集合中的其他缓冲区。简单的哈希算法往往不能满足单调性的要求，如最简单的线性哈希：x = (ax + b) mod (P)，在上式中，P表示全部缓冲的大小。不难看出，当缓冲大小发生变化时(从P1到P2)，原来所有的哈希结果均会发生变化，从而不满足单调性的要求。哈希结果的变化意味着当缓冲空间发生变化时，所有的映射关系需要在系统内全部更新。而在P2P系统内，缓冲的变化等价于Peer加入或退出系统，这一情况在P2P系统中会频繁发生，因此会带来极大计算和传输负荷。单调性就是要求哈希算法能够应对这种情况。
- **分散性 (Spread)**: 在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。
- **负载 (Load)**: 负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。
- **平滑性 (Smoothness)**: 平滑性是指缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的。

## 3. 原理
### (1) 基本概念

一致性哈希算法（Consistent Hashing）最早在论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》中被提出。简单来说，一致性哈希将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-2^32-1（即哈希值是一个32位无符号整形），整个哈希空间环如下：

![](https://images2015.cnblogs.com/blog/498077/201608/498077-20160822172453355-790656043.png)

整个空间按顺时针方向组织。0 和 2^32-1 在零点中方向重合。  
下一步将各个服务器使用Hash进行一个哈希，具体可以选择服务器的ip或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用ip地址哈希后在环空间的位置如下：

![](https://images2015.cnblogs.com/blog/498077/201608/498077-20160822172523808-1567363338.png)

接下来使用如下算法定位数据访问到相应服务器：将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。  
例如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下：

![](https://images2015.cnblogs.com/blog/498077/201608/498077-20160822172807745-742859090.png)

根据一致性哈希算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。  
下面分析一致性哈希算法的容错性和可扩展性。现假设Node C不幸宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性哈希算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。  
下面考虑另外一种情况，如果在系统中增加一台服务器Node X，如下图所示：

![](https://images2015.cnblogs.com/blog/498077/201608/498077-20160822172901526-169091807.png)

此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X 。一般的，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。  
综上所述，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。  
另外，一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。例如系统中只有两台服务器，其环分布如下，

![](https://images2015.cnblogs.com/blog/498077/201608/498077-20160822172922917-1331181630.png)

此时必然造成大量数据集中到Node A上，而只有极少量会定位到Node B上。为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器ip或主机名的后面增加编号来实现。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点：

![](https://images2015.cnblogs.com/blog/498077/201608/498077-20160822172943917-133540408.png)

同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。

# 六. Collections.sort 底层的排序方式

# 七. 排序的稳定性，以及不同场景下的排序策略

# 八. 阻塞队列不用 Java 提供的该怎么实现

